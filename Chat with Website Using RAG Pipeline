# Import Libraries
import requests
from bs4 import BeautifulSoup
import openai
import faiss
import numpy as np

# Set your OpenAI API key here
openai.api_key = 'your-openai-api-key'

##########################
# 1. Website Scraping
##########################

def scrape_website(url):
    """
    Scrapes text content from a website's <p> tags.
    Args:
        url (str): The URL of the website to scrape.
    Returns:
        str: Combined text content.
    """
    response = requests.get(url)
    if response.status_code != 200:
        raise Exception(f"Failed to fetch website: {url}")
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract text from all paragraph tags
    paragraphs = soup.find_all('p')
    text_content = ' '.join([para.get_text() for para in paragraphs])
    return text_content

# Example: Provide the website URL
url = "https://example.com"
print("Scraping website content...")
website_text = scrape_website(url)
print("Scraping complete!")

##########################
# 2. Data Chunking
##########################

def chunk_text(text, chunk_size=500):
    """
    Splits the text into smaller chunks of fixed size.
    Args:
        text (str): Input text.
        chunk_size (int): Number of words per chunk.
    Returns:
        list: List of text chunks.
    """
    words = text.split()
    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

# Chunk the scraped content
print("Chunking text into smaller segments...")
chunks = chunk_text(website_text)
print(f"Created {len(chunks)} chunks of text.")

##########################
# 3. Embedding Generation
##########################

def get_embedding(text):
    """
    Generates an embedding vector for the given text using OpenAI's embedding model.
    Args:
        text (str): Input text.
    Returns:
        list: Embedding vector.
    """
    response = openai.Embedding.create(input=text, model="text-embedding-ada-002")
    return response['data'][0]['embedding']

# Generate embeddings for all chunks
print("Generating embeddings for text chunks...")
embeddings = [get_embedding(chunk) for chunk in chunks]
print("Embedding generation complete!")

##########################
# 4. FAISS Vector Store
##########################

def create_faiss_index(embeddings):
    """
    Creates a FAISS index and adds embeddings.
    Args:
        embeddings (list): List of embedding vectors.
    Returns:
        faiss.IndexFlatL2: FAISS index object.
    """
    embedding_dim = len(embeddings[0])  # Dimension of embedding vectors
    index = faiss.IndexFlatL2(embedding_dim)  # L2 distance
    embedding_array = np.array(embeddings).astype('float32')
    index.add(embedding_array)
    return index

# Create and populate the FAISS index
print("Storing embeddings in FAISS vector database...")
faiss_index = create_faiss_index(embeddings)
print("FAISS index created successfully!")

##########################
# 5. Query Handling
##########################

def query_rag_pipeline(query, index, chunks, top_k=3):
    """
    Handles user queries using the RAG pipeline.
    Args:
        query (str): User's natural language question.
        index (faiss.IndexFlatL2): FAISS index.
        chunks (list): List of text chunks.
        top_k (int): Number of relevant chunks to retrieve.
    Returns:
        str: Response generated by GPT model.
    """
    # Generate query embedding
    query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)
    
    # Search for top_k similar chunks
    distances, indices = index.search(query_embedding, top_k)
    
    # Retrieve relevant chunks
    retrieved_chunks = [chunks[i] for i in indices[0]]
    context = ' '.join(retrieved_chunks)
    
    # Generate a response using OpenAI GPT model
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"
    response = openai.Completion.create(engine="text-davinci-003", prompt=prompt, max_tokens=150)
    
    return response.choices[0].text.strip()

# Example: User query
user_query = "What is the main content of the website?"
print("Processing user query...")
response = query_rag_pipeline(user_query, faiss_index, chunks)
print("\nGenerated Response:")
print(response)
